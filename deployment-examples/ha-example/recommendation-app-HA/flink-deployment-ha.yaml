apiVersion: flink.apache.org/v1beta1
kind: FlinkDeployment
metadata:
  name: recommendation-app
spec:
  image: quay.io/streamshub/flink-sql-runner:v0.0.1
  flinkVersion: v1_19
  flinkConfiguration:
    taskmanager.numberOfTaskSlots: "1"
    metrics.reporters: prom
    metrics.reporter.prom.factory.class: org.apache.flink.metrics.prometheus.PrometheusReporterFactory
    # job manager HA settings
    high-availability.type: KUBERNETES
    high-availability.storageDir: s3://test/ha
    kubernetes.cluster-id: ha-test
    # task checkpointing settings
    execution.checkpointing.interval: 1min
    execution.checkpointing.externalized-checkpoint-retention: RETAIN_ON_CANCELLATION
    execution.checkpointing.unaligned.enabled: "false"
    execution.checkpointing.filesystem: storage
    state.checkpoints.dir: s3://test/cp
    execution.checkpointing.dir: s3://test/cp
    # S3 (minio) connection settings
    s3.access-key: minioadmin
    s3.secret-key: minioadmin
    s3.endpoint: http://MINIO_POD_ID:9000
    s3.path.style.access: "true"
  serviceAccount: flink
  podTemplate:
    kind: Pod
    metadata:
      name: recommendation-app
    spec:
      containers:
        - name: flink-main-container
          imagePullPolicy: Always
          ports:
            - containerPort: 9249
              name: prom
          volumeMounts:
            - name: product-inventory-vol
              mountPath: /opt/flink/data
          env:
            - name: ENABLE_BUILT_IN_PLUGINS
              value: flink-s3-fs-hadoop-1.19.1.jar;flink-s3-fs-presto-1.19.1.jar
      volumes:
        - name: product-inventory-vol
          configMap:
            name: product-inventory
            items:
              - key: productInventory.csv
                path: productInventory.csv
  jobManager:
    resource:
      memory: "2048m"
      cpu: 1
    replicas: 2
  taskManager:
    resource:
      memory: "2048m"
      cpu: 1
  job:
    jarURI: local:///opt/streamshub/flink-sql-runner.jar
    args: ["
        CREATE TABLE ProductInventoryTable (
          product_id STRING, 
          category STRING, 
          stock STRING, 
          rating STRING 
        ) WITH (
         'connector' = 'filesystem', 
         'path' = '/opt/flink/data/productInventory.csv', 
         'format' = 'csv', 
         'csv.ignore-parse-errors' = 'true' 
        ); 
        CREATE TABLE ClickStreamTable (
          user_id STRING, 
          product_id STRING, 
          `event_time` TIMESTAMP(3) METADATA FROM 'timestamp', 
          WATERMARK FOR event_time AS event_time - INTERVAL '1' SECOND 
        ) WITH ( 
          'connector' = 'kafka', 
          'topic' = 'flink.click.streams', 
          'properties.bootstrap.servers' = 
          'my-cluster-kafka-bootstrap.flink.svc:9092', 
          'properties.group.id' = 'click-stream-group', 
          'value.format' = 'avro-confluent', 
          'value.avro-confluent.url' = 'http://apicurio-registry-service.flink.svc:8080/apis/ccompat/v6', 
          'scan.startup.mode' = 'latest-offset' ); 
        CREATE TABLE SalesRecordTable (
          invoice_id STRING, 
          user_id STRING, 
          product_id STRING, 
          quantity STRING, 
          unit_cost STRING, 
          `purchase_time` TIMESTAMP(3) METADATA FROM 'timestamp', 
          WATERMARK FOR purchase_time AS purchase_time - INTERVAL '1' SECOND 
        ) WITH (
          'connector' = 'kafka', 
          'topic' = 'flink.sales.records', 
          'properties.bootstrap.servers' = 'my-cluster-kafka-bootstrap.flink.svc:9092',
          'properties.group.id' = 'sales-record-group', 
          'value.format' = 'avro-confluent', 
          'value.avro-confluent.url' = 'http://apicurio-registry-service.flink.svc:8080/apis/ccompat/v6', 
          'scan.startup.mode' = 'latest-offset' 
        ); 
        CREATE TABLE CsvSinkTable ( 
          user_id STRING, 
          top_product_ids STRING, 
          `event_time` TIMESTAMP(3), 
          PRIMARY KEY(`user_id`) NOT ENFORCED 
        ) WITH ( 
          'connector' = 'upsert-kafka', 
          'topic' = 'flink.recommended.products', 
          'properties.bootstrap.servers' = 'my-cluster-kafka-bootstrap.flink.svc:9092', 
          'properties.client.id' = 'recommended-products-producer-client', 
          'properties.transaction.timeout.ms' = '800000', 'key.format' = 'csv', 
          'value.format' = 'csv', 'value.fields-include' = 'ALL' 
        ); 
        CREATE TEMPORARY VIEW clicked_products AS 
          SELECT DISTINCT 
            c.user_id, c.event_time, 
            p.product_id, 
            p.category 
          FROM ClickStreamTable AS c 
          JOIN ProductInventoryTable AS p ON c.product_id = p.product_id
        ; 
        CREATE TEMPORARY VIEW category_products AS 
          SELECT 
            cp.user_id, 
            cp.event_time, 
            p.product_id, 
            p.category, 
            p.stock, 
            p.rating, 
            sr.user_id as purchased 
          FROM clicked_products cp 
          JOIN ProductInventoryTable AS p ON cp.category = p.category 
          LEFT JOIN SalesRecordTable sr ON cp.user_id = sr.user_id AND p.product_id = sr.product_id 
          WHERE p.stock > 0 
          GROUP BY 
            p.product_id, 
            p.category, 
            p.stock, 
            cp.user_id, 
            cp.event_time, 
            sr.user_id, 
            p.rating
        ; 
        CREATE TEMPORARY VIEW top_products AS 
          SELECT cp.user_id, 
            cp.event_time, 
            cp.product_id, 
            cp.category, 
            cp.stock, 
            cp.rating, 
            cp.purchased, 
          ROW_NUMBER() OVER (PARTITION BY cp.user_id ORDER BY cp.purchased DESC, cp.rating DESC) AS rn 
          FROM category_products cp
        ; 
        INSERT INTO CsvSinkTable 
          SELECT 
            user_id, 
            LISTAGG(product_id, ',') AS top_product_ids, 
            TUMBLE_END(event_time, INTERVAL '5' SECOND) 
          FROM top_products 
          WHERE rn <= 6 
          GROUP BY user_id, 
          TUMBLE(event_time, INTERVAL '5' SECOND)
        ;
      "]
    parallelism: 1
    upgradeMode: stateless
